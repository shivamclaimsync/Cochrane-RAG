{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive Weaviate Tutorial for RAG Systems\n",
        "\n",
        "This tutorial covers everything you need to know about Weaviate to build production-ready RAG (Retrieval-Augmented Generation) systems.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction to Weaviate & Vector Databases](#1-introduction)\n",
        "2. [Setup & Connection](#2-setup-connection)\n",
        "3. [Schema Design & Collections](#3-schema-design)\n",
        "4. [Data Insertion (CRUD Operations)](#4-crud-operations)\n",
        "5. [Vector Search & Similarity](#5-vector-search)\n",
        "6. [Filtering & Hybrid Search](#6-filtering-hybrid)\n",
        "7. [Batch Operations](#7-batch-operations)\n",
        "8. [RAG-Specific Patterns](#8-rag-patterns)\n",
        "9. [Advanced Features](#9-advanced-features)\n",
        "10. [Best Practices & Optimization](#10-best-practices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Introduction to Weaviate & Vector Databases\n",
        "\n",
        "### What is Weaviate?\n",
        "Weaviate is an **open-source vector database** designed for storing, indexing, and searching high-dimensional vector embeddings alongside traditional data.\n",
        "\n",
        "### Why Use Weaviate for RAG?\n",
        "- **Fast semantic search**: Find similar content based on meaning, not just keywords\n",
        "- **Hybrid search**: Combine vector similarity with traditional filters\n",
        "- **Scalability**: Handle millions of vectors efficiently\n",
        "- **Built-in vectorization**: Can use various embedding models (OpenAI, Cohere, Hugging Face, etc.)\n",
        "- **Multi-modal**: Support for text, images, and more\n",
        "\n",
        "### Key Concepts\n",
        "- **Vector Embeddings**: Numerical representations of data (text, images) that capture semantic meaning\n",
        "- **Collections (Classes)**: Like tables in traditional databases, they define the schema\n",
        "- **Objects**: Individual data items stored in collections\n",
        "- **Properties**: Attributes of objects (both vector and scalar)\n",
        "- **ANN (Approximate Nearest Neighbor)**: The algorithm used for fast similarity search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Setup & Connection\n",
        "\n",
        "### Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment to run)\n",
        "# !pip install weaviate-client python-dotenv openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connection Methods\n",
        "\n",
        "Weaviate offers multiple deployment options:\n",
        "1. **Weaviate Cloud (WCS)** - Managed cloud service\n",
        "2. **Docker** - Local deployment\n",
        "3. **Kubernetes** - Production deployment\n",
        "4. **Embedded Weaviate** - In-process for development\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Successfully connected to Weaviate!\n",
            "ğŸ“Š Weaviate is ready at: ip2ssdretwojl8szzqk4ew.c0.asia-southeast1.gcp.weaviate.cloud\n"
          ]
        }
      ],
      "source": [
        "import weaviate\n",
        "from weaviate.classes.init import Auth\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Method 1: Weaviate Cloud Service (WCS)\n",
        "def connect_to_wcs():\n",
        "    \"\"\"Connect to Weaviate Cloud\"\"\"\n",
        "    weaviate_url = os.environ[\"WEAVIATE_URL\"]\n",
        "    weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
        "    \n",
        "    client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=weaviate_url,  # Replace with your Weaviate Cloud URL\n",
        "    auth_credentials=Auth.api_key(weaviate_api_key),  # Replace with your Weaviate Cloud key\n",
        ")\n",
        "    return client\n",
        "\n",
        "# Method 2: Local Docker Instance\n",
        "def connect_to_local():\n",
        "    \"\"\"Connect to local Weaviate instance\"\"\"\n",
        "    client = weaviate.connect_to_local(\n",
        "        host=\"localhost\",\n",
        "        port=8080,\n",
        "    )\n",
        "    return client\n",
        "\n",
        "# Connect to your Weaviate instance\n",
        "client = connect_to_wcs()  # Change based on your setup\n",
        "\n",
        "# Check connection\n",
        "if client.is_ready():\n",
        "    print(\"âœ… Successfully connected to Weaviate!\")\n",
        "    print(f\"ğŸ“Š Weaviate is ready at: {os.getenv('WEAVIATE_URL')}\")\n",
        "else:\n",
        "    print(\"âŒ Failed to connect to Weaviate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Schema Design & Collections\n",
        "\n",
        "### Understanding Collections (Classes)\n",
        "\n",
        "A collection defines:\n",
        "- **Name**: Identifier for the collection\n",
        "- **Properties**: Data fields with types\n",
        "- **Vectorizer**: How to generate embeddings\n",
        "- **Vector Index**: Configuration for similarity search\n",
        "- **Replication**: Data redundancy settings\n",
        "\n",
        "### Data Types in Weaviate\n",
        "\n",
        "| Data Type | Description | Example Use Case |\n",
        "|-----------|-------------|------------------|\n",
        "| `TEXT` | String data | Content, titles, names |\n",
        "| `TEXT_ARRAY` | Array of strings | Tags, keywords, categories |\n",
        "| `INT` | Integer | Counts, indices, IDs |\n",
        "| `NUMBER` | Float | Scores, ratings, percentages |\n",
        "| `BOOL` | Boolean | Flags, status indicators |\n",
        "| `DATE` | ISO 8601 date | Timestamps, publication dates |\n",
        "| `UUID` | Unique identifier | References, links |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from weaviate.classes.config import Configure, Property, DataType, VectorDistances\n",
        "\n",
        "# Example: Advanced RAG Collection with Rich Metadata\n",
        "def create_rag_collection(client):\n",
        "    \"\"\"Create a comprehensive RAG collection with rich metadata\"\"\"\n",
        "    \n",
        "    # Delete if exists (for tutorial purposes)\n",
        "    try:\n",
        "        client.collections.delete(\"RAGChunk\")\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    client.collections.create(\n",
        "        name=\"RAGChunk\",\n",
        "        properties=[\n",
        "            # Content fields\n",
        "            Property(name=\"chunk_id\", data_type=DataType.TEXT),\n",
        "            Property(name=\"content\", data_type=DataType.TEXT),\n",
        "            Property(name=\"title\", data_type=DataType.TEXT),\n",
        "            \n",
        "            # Hierarchy fields\n",
        "            Property(name=\"document_id\", data_type=DataType.TEXT),\n",
        "            Property(name=\"section_name\", data_type=DataType.TEXT),\n",
        "            Property(name=\"parent_chunk_id\", data_type=DataType.TEXT),\n",
        "            Property(name=\"chunk_level\", data_type=DataType.TEXT),  # document, section, paragraph\n",
        "            \n",
        "            # Numerical metadata\n",
        "            Property(name=\"chunk_index\", data_type=DataType.INT),\n",
        "            Property(name=\"word_count\", data_type=DataType.INT),\n",
        "            \n",
        "            # Boolean flags\n",
        "            Property(name=\"has_code\", data_type=DataType.BOOL),\n",
        "            Property(name=\"has_table\", data_type=DataType.BOOL),\n",
        "            \n",
        "            # Additional metadata\n",
        "            Property(name=\"tags\", data_type=DataType.TEXT_ARRAY),\n",
        "            Property(name=\"created_at\", data_type=DataType.DATE),\n",
        "        ],\n",
        "        # Use no vectorizer - we'll provide our own embeddings\n",
        "        vectorizer_config=Configure.Vectorizer.none(),\n",
        "        \n",
        "        # Vector index configuration (HNSW is default and recommended)\n",
        "        vector_index_config=Configure.VectorIndex.hnsw(\n",
        "            distance_metric=VectorDistances.COSINE,  # or DOT, L2_SQUARED\n",
        "            ef_construction=128,  # Higher = better quality, slower indexing\n",
        "            max_connections=64,   # Higher = better recall, more memory\n",
        "        ),\n",
        "    )\n",
        "    print(\"âœ… Created RAGChunk collection with advanced configuration\")\n",
        "\n",
        "# Create the collection\n",
        "create_rag_collection(client)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. CRUD Operations (Create, Read, Update, Delete)\n",
        "\n",
        "### Create - Inserting Data\n",
        "\n",
        "The most common operation in RAG is inserting document chunks with their embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import uuid\n",
        "\n",
        "# Get the collection\n",
        "rag_chunks = client.collections.get(\"RAGChunk\")\n",
        "\n",
        "# For this tutorial, we'll use dummy vectors\n",
        "# In a real RAG system, you'd generate embeddings using OpenAI, Cohere, etc.\n",
        "dummy_vector = np.random.rand(1536).tolist()  # 1536 is OpenAI's embedding dimension\n",
        "\n",
        "# Insert a single object WITH vector\n",
        "uuid_obj = rag_chunks.data.insert(\n",
        "    properties={\n",
        "        \"chunk_id\": \"chunk_001\",\n",
        "        \"content\": \"Retrieval-Augmented Generation (RAG) combines the power of large language models with external knowledge bases to provide more accurate and contextual responses.\",\n",
        "        \"title\": \"Introduction to RAG\",\n",
        "        \"document_id\": \"doc_001\",\n",
        "        \"section_name\": \"Overview\",\n",
        "        \"parent_chunk_id\": \"\",\n",
        "        \"chunk_level\": \"paragraph\",\n",
        "        \"chunk_index\": 0,\n",
        "        \"word_count\": 25,\n",
        "        \"has_code\": False,\n",
        "        \"has_table\": False,\n",
        "        \"tags\": [\"RAG\", \"introduction\", \"AI\"],\n",
        "        \"created_at\": datetime.now().isoformat(),\n",
        "    },\n",
        "    vector=dummy_vector  # Your actual embedding goes here\n",
        ")\n",
        "\n",
        "print(f\"âœ… Inserted object with UUID: {uuid_obj}\")\n",
        "print(f\"   Content: {rag_chunks.query.fetch_object_by_id(uuid_obj).properties['content'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read - Fetching Objects\n",
        "\n",
        "Multiple ways to retrieve data from Weaviate:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Fetch by UUID\n",
        "obj = rag_chunks.query.fetch_object_by_id(uuid_obj)\n",
        "print(\"ğŸ“– Retrieved object by UUID:\")\n",
        "print(f\"   Title: {obj.properties['title']}\")\n",
        "print(f\"   Content: {obj.properties['content'][:100]}...\")\n",
        "\n",
        "# Method 2: Fetch multiple objects (simple query)\n",
        "response = rag_chunks.query.fetch_objects(\n",
        "    limit=10,\n",
        "    include_vector=False  # Set to True if you need the embedding back\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“š Retrieved {len(response.objects)} objects total\")\n",
        "for i, obj in enumerate(response.objects, 1):\n",
        "    print(f\"   {i}. {obj.properties['title']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Vector Search - The Core of RAG\n",
        "\n",
        "Vector search is the **most important feature** for RAG systems. It finds semantically similar content based on meaning rather than exact keyword matches.\n",
        "\n",
        "### How Vector Search Works\n",
        "1. Convert your query to an embedding (vector)\n",
        "2. Search for vectors in the database that are \"close\" to your query vector\n",
        "3. Return the most similar results\n",
        "\n",
        "Let's add some sample data and perform searches:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's add more sample documents about AI topics\n",
        "sample_chunks = [\n",
        "    {\n",
        "        \"content\": \"Machine learning is a subset of artificial intelligence that focuses on learning from data to make predictions or decisions without being explicitly programmed.\",\n",
        "        \"title\": \"ML Basics\",\n",
        "        \"section\": \"Introduction\",\n",
        "        \"tags\": [\"machine learning\", \"AI\", \"basics\"]\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Deep learning uses neural networks with multiple layers to learn complex patterns from large amounts of data. It has revolutionized computer vision and NLP.\",\n",
        "        \"title\": \"Deep Learning Overview\",\n",
        "        \"section\": \"Advanced Topics\",\n",
        "        \"tags\": [\"deep learning\", \"neural networks\", \"AI\"]\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Natural language processing (NLP) enables computers to understand, interpret, and generate human language in a valuable way.\",\n",
        "        \"title\": \"NLP Fundamentals\",\n",
        "        \"section\": \"NLP\",\n",
        "        \"tags\": [\"NLP\", \"language\", \"processing\"]\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Computer vision allows machines to interpret and understand visual information from images and videos, enabling applications like facial recognition and autonomous vehicles.\",\n",
        "        \"title\": \"Computer Vision\",\n",
        "        \"section\": \"Vision\",\n",
        "        \"tags\": [\"computer vision\", \"images\", \"AI\"]\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Reinforcement learning trains agents to make decisions through trial and error, learning from rewards and penalties in an environment.\",\n",
        "        \"title\": \"RL Basics\",\n",
        "        \"section\": \"RL\",\n",
        "        \"tags\": [\"reinforcement learning\", \"agents\", \"AI\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Insert sample chunks with dummy embeddings\n",
        "inserted_ids = []\n",
        "for i, chunk in enumerate(sample_chunks):\n",
        "    # In a real system, generate embeddings using: \n",
        "    # - OpenAI: openai.embeddings.create()\n",
        "    # - Sentence Transformers: model.encode()\n",
        "    # - Cohere: co.embed()\n",
        "    dummy_vector = np.random.rand(1536).tolist()\n",
        "    \n",
        "    uuid_inserted = rag_chunks.data.insert(\n",
        "        properties={\n",
        "            \"chunk_id\": f\"chunk_{i+1}\",\n",
        "            \"content\": chunk[\"content\"],\n",
        "            \"title\": chunk[\"title\"],\n",
        "            \"document_id\": \"ai_guide\",\n",
        "            \"section_name\": chunk[\"section\"],\n",
        "            \"parent_chunk_id\": \"\",\n",
        "            \"chunk_level\": \"paragraph\",\n",
        "            \"chunk_index\": i+1,\n",
        "            \"word_count\": len(chunk[\"content\"].split()),\n",
        "            \"has_code\": False,\n",
        "            \"has_table\": False,\n",
        "            \"tags\": chunk[\"tags\"],\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "        },\n",
        "        vector=dummy_vector\n",
        "    )\n",
        "    inserted_ids.append(uuid_inserted)\n",
        "\n",
        "print(f\"âœ… Inserted {len(inserted_ids)} sample chunks for vector search\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Vector Search (Near Vector)\n",
        "# This is the CORE operation for RAG!\n",
        "\n",
        "# Simulate a query embedding (in reality, you'd embed the user's question)\n",
        "# Example question: \"How do neural networks learn patterns?\"\n",
        "query_vector = np.random.rand(1536).tolist()\n",
        "\n",
        "response = rag_chunks.query.near_vector(\n",
        "    near_vector=query_vector,\n",
        "    limit=3,  # Top 3 most similar results\n",
        "    return_metadata=['distance', 'certainty'],  # Include similarity scores\n",
        ")\n",
        "\n",
        "print(\"ğŸ” Vector Search Results (Top 3 most similar):\\n\")\n",
        "for i, obj in enumerate(response.objects, 1):\n",
        "    print(f\"{i}. {obj.properties['title']}\")\n",
        "    print(f\"   Content: {obj.properties['content'][:100]}...\")\n",
        "    print(f\"   Distance: {obj.metadata.distance:.4f} (lower = more similar)\")\n",
        "    print(f\"   Certainty: {obj.metadata.certainty:.4f} (higher = more confident)\")\n",
        "    print(f\"   Tags: {', '.join(obj.properties['tags'])}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Distance Metrics\n",
        "\n",
        "Weaviate supports multiple distance metrics:\n",
        "\n",
        "| Metric | Range | Best For | Formula |\n",
        "|--------|-------|----------|---------|\n",
        "| **Cosine** | 0-2 | Most common, normalized vectors | 1 - cos(Î¸) |\n",
        "| **Dot Product** | -âˆ to +âˆ | Already normalized vectors | -aÂ·b |\n",
        "| **L2 (Euclidean)** | 0 to +âˆ | When magnitude matters | âˆšÎ£(aáµ¢-báµ¢)Â² |\n",
        "\n",
        "**Key Concepts:**\n",
        "- **Distance**: Raw metric (lower = more similar)\n",
        "- **Certainty**: Normalized 0-1 score (higher = more similar)\n",
        "- **Cosine distance** is most popular because it measures angle, not magnitude\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Filtering & Hybrid Search\n",
        "\n",
        "One of the most powerful features for RAG is combining **semantic search** with **metadata filters**.\n",
        "\n",
        "### Why Filter?\n",
        "- Restrict search to specific documents, sections, or time periods\n",
        "- Filter by quality scores, content types, or custom metadata\n",
        "- Improve relevance by combining semantic similarity with business logic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from weaviate.classes.query import Filter\n",
        "\n",
        "# Example 1: Simple Filter (exact match)\n",
        "print(\"ğŸ“Š Example 1: Filter by section_name\\n\")\n",
        "response = rag_chunks.query.fetch_objects(\n",
        "    filters=Filter.by_property(\"section_name\").equal(\"Introduction\"),\n",
        "    limit=10\n",
        ")\n",
        "\n",
        "print(f\"Found {len(response.objects)} chunks in 'Introduction' section:\")\n",
        "for obj in response.objects:\n",
        "    print(f\"  - {obj.properties['title']}\")\n",
        "\n",
        "# Example 2: Complex Filters with AND/OR\n",
        "print(\"\\nğŸ“Š Example 2: Complex Filter (word_count > 20 AND has_code = False)\\n\")\n",
        "response = rag_chunks.query.fetch_objects(\n",
        "    filters=(\n",
        "        Filter.by_property(\"word_count\").greater_than(20) &\n",
        "        Filter.by_property(\"has_code\").equal(False)\n",
        "    ),\n",
        "    limit=10\n",
        ")\n",
        "\n",
        "print(f\"Found {len(response.objects)} chunks:\")\n",
        "for obj in response.objects:\n",
        "    print(f\"  - {obj.properties['title']} ({obj.properties['word_count']} words)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Vector Search WITH Filters - THE POWER COMBO FOR RAG! ğŸ¯\n",
        "print(\"\\nğŸ¯ Example 3: Vector Search + Filters (most powerful for RAG!)\\n\")\n",
        "\n",
        "query_vector = np.random.rand(1536).tolist()\n",
        "\n",
        "response = rag_chunks.query.near_vector(\n",
        "    near_vector=query_vector,\n",
        "    filters=(\n",
        "        Filter.by_property(\"document_id\").equal(\"ai_guide\") &\n",
        "        Filter.by_property(\"section_name\").not_equal(\"Introduction\")\n",
        "    ),\n",
        "    limit=5,\n",
        "    return_metadata=['distance']\n",
        ")\n",
        "\n",
        "print(\"Results (semantic search + metadata filters):\")\n",
        "for i, obj in enumerate(response.objects, 1):\n",
        "    print(f\"{i}. {obj.properties['title']}\")\n",
        "    print(f\"   Section: {obj.properties['section_name']}\")\n",
        "    print(f\"   Distance: {obj.metadata.distance:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Available Filter Operators\n",
        "\n",
        "```python\n",
        "# Comparison operators\n",
        "Filter.by_property(\"age\").equal(25)\n",
        "Filter.by_property(\"score\").not_equal(0)\n",
        "Filter.by_property(\"count\").greater_than(100)\n",
        "Filter.by_property(\"count\").greater_or_equal(100)\n",
        "Filter.by_property(\"count\").less_than(100)\n",
        "Filter.by_property(\"count\").less_or_equal(100)\n",
        "\n",
        "# Text operators\n",
        "Filter.by_property(\"title\").like(\"*machine*\")  # Wildcard\n",
        "Filter.by_property(\"tags\").contains_any([\"AI\", \"ML\"])  # Array contains any\n",
        "Filter.by_property(\"tags\").contains_all([\"AI\", \"ML\"])  # Array contains all\n",
        "\n",
        "# Boolean operators\n",
        "filter1 & filter2  # AND\n",
        "filter1 | filter2  # OR\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Batch Operations\n",
        "\n",
        "For RAG systems, you'll often need to insert **thousands of chunks**. Batching is essential for performance!\n",
        "\n",
        "### Why Batch?\n",
        "- **Speed**: 10-100x faster than individual inserts\n",
        "- **Efficiency**: Reduces network overhead\n",
        "- **Reliability**: Built-in error handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Method 1: Dynamic Batching (RECOMMENDED)\n",
        "# Weaviate automatically optimizes batch size based on performance\n",
        "print(\"âš¡ Dynamic Batching (Recommended)\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with rag_chunks.batch.dynamic() as batch:\n",
        "    for i in range(50):  # Insert 50 objects\n",
        "        dummy_vector = np.random.rand(1536).tolist()\n",
        "        \n",
        "        batch.add_object(\n",
        "            properties={\n",
        "                \"chunk_id\": f\"batch_chunk_{i}\",\n",
        "                \"content\": f\"This is batch content number {i} with meaningful information about AI and machine learning.\",\n",
        "                \"title\": f\"Batch Document {i}\",\n",
        "                \"document_id\": f\"doc_batch_{i // 10}\",  # 10 chunks per document\n",
        "                \"section_name\": \"Main\",\n",
        "                \"parent_chunk_id\": \"\",\n",
        "                \"chunk_level\": \"paragraph\",\n",
        "                \"chunk_index\": i,\n",
        "                \"word_count\": 15,\n",
        "                \"has_code\": False,\n",
        "                \"has_table\": False,\n",
        "                \"tags\": [\"batch\", \"test\"],\n",
        "                \"created_at\": datetime.now().isoformat(),\n",
        "            },\n",
        "            vector=dummy_vector\n",
        "        )\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"âœ… Batch inserted 50 objects in {elapsed:.2f} seconds\")\n",
        "print(f\"   Rate: {50/elapsed:.1f} objects/second\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Fixed-Size Batching\n",
        "# Use when you want more control over batch size\n",
        "print(\"\\nâš¡ Fixed-Size Batching\\n\")\n",
        "\n",
        "with rag_chunks.batch.fixed_size(\n",
        "    batch_size=20,  # Send every 20 objects\n",
        "    concurrent_requests=2  # Parallel uploads\n",
        ") as batch:\n",
        "    for i in range(40):\n",
        "        batch.add_object(\n",
        "            properties={\n",
        "                \"chunk_id\": f\"fixed_chunk_{i}\",\n",
        "                \"content\": f\"Fixed batch content {i}\",\n",
        "                \"title\": f\"Fixed Batch {i}\",\n",
        "                \"document_id\": \"doc_fixed\",\n",
        "                \"section_name\": \"Test\",\n",
        "                \"parent_chunk_id\": \"\",\n",
        "                \"chunk_level\": \"paragraph\",\n",
        "                \"chunk_index\": i,\n",
        "                \"word_count\": 5,\n",
        "                \"has_code\": False,\n",
        "                \"has_table\": False,\n",
        "                \"tags\": [\"fixed\"],\n",
        "                \"created_at\": datetime.now().isoformat(),\n",
        "            },\n",
        "            vector=np.random.rand(1536).tolist()\n",
        "        )\n",
        "\n",
        "print(\"âœ… Fixed-size batch insertion complete\")\n",
        "\n",
        "# Check total count\n",
        "agg = rag_chunks.aggregate.over_all(total_count=True)\n",
        "print(f\"\\nğŸ“Š Total chunks in database: {agg.total_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Size Guidelines\n",
        "\n",
        "Choose batch size based on your object size:\n",
        "\n",
        "| Object Size | Recommended Batch Size |\n",
        "|-------------|----------------------|\n",
        "| Small (<1KB) | 500-1000 |\n",
        "| Medium (1-10KB) | 100-200 |\n",
        "| Large (>10KB) | 20-50 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. RAG-Specific Patterns\n",
        "\n",
        "These are advanced retrieval patterns specifically designed for RAG systems.\n",
        "\n",
        "### Pattern 1: Hierarchical Retrieval\n",
        "\n",
        "Store chunks at multiple levels (document â†’ section â†’ paragraph) and retrieve with context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hierarchical_retrieval(client, query_vector, document_id=None, top_k=5):\n",
        "    \"\"\"\n",
        "    Hierarchical retrieval: Find relevant paragraphs, then retrieve their parent sections.\n",
        "    This provides more context for the LLM.\n",
        "    \"\"\"\n",
        "    rag_chunks = client.collections.get(\"RAGChunk\")\n",
        "    \n",
        "    # Step 1: Find relevant paragraphs using vector search\n",
        "    filters = Filter.by_property(\"chunk_level\").equal(\"paragraph\")\n",
        "    if document_id:\n",
        "        filters = filters & Filter.by_property(\"document_id\").equal(document_id)\n",
        "    \n",
        "    paragraph_results = rag_chunks.query.near_vector(\n",
        "        near_vector=query_vector,\n",
        "        filters=filters,\n",
        "        limit=top_k\n",
        "    )\n",
        "    \n",
        "    print(f\"ğŸ“„ Hierarchical Retrieval Results:\\n\")\n",
        "    print(f\"Found {len(paragraph_results.objects)} relevant paragraphs:\")\n",
        "    for i, obj in enumerate(paragraph_results.objects, 1):\n",
        "        print(f\"  {i}. {obj.properties['title']}\")\n",
        "        print(f\"     Section: {obj.properties['section_name']}\")\n",
        "        print(f\"     Content: {obj.properties['content'][:80]}...\\n\")\n",
        "    \n",
        "    return paragraph_results.objects\n",
        "\n",
        "# Example usage\n",
        "query_vec = np.random.rand(1536).tolist()\n",
        "results = hierarchical_retrieval(client, query_vec, document_id=\"ai_guide\", top_k=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pattern 2: Metadata-Enhanced Retrieval\n",
        "\n",
        "Use rich metadata to improve retrieval quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def enhanced_rag_retrieval(client, query_vector, filters_dict=None, top_k=5):\n",
        "    \"\"\"\n",
        "    Advanced RAG retrieval with multiple filters and metadata.\n",
        "    \n",
        "    Args:\n",
        "        client: Weaviate client\n",
        "        query_vector: Query embedding\n",
        "        filters_dict: Dict of property:value filters\n",
        "        top_k: Number of results\n",
        "    \"\"\"\n",
        "    rag_chunks = client.collections.get(\"RAGChunk\")\n",
        "    \n",
        "    # Build combined filter\n",
        "    combined_filter = None\n",
        "    if filters_dict:\n",
        "        for key, value in filters_dict.items():\n",
        "            new_filter = Filter.by_property(key).equal(value)\n",
        "            if combined_filter is None:\n",
        "                combined_filter = new_filter\n",
        "            else:\n",
        "                combined_filter = combined_filter & new_filter\n",
        "    \n",
        "    # Retrieve with filters\n",
        "    results = rag_chunks.query.near_vector(\n",
        "        near_vector=query_vector,\n",
        "        filters=combined_filter,\n",
        "        limit=top_k,\n",
        "        return_metadata=['distance']\n",
        "    )\n",
        "    \n",
        "    # Format results with rich metadata\n",
        "    retrieved_chunks = []\n",
        "    for obj in results.objects:\n",
        "        retrieved_chunks.append({\n",
        "            'content': obj.properties['content'],\n",
        "            'metadata': {\n",
        "                'title': obj.properties['title'],\n",
        "                'section': obj.properties['section_name'],\n",
        "                'document_id': obj.properties['document_id'],\n",
        "                'tags': obj.properties.get('tags', []),\n",
        "                'word_count': obj.properties['word_count'],\n",
        "                'distance': obj.metadata.distance,\n",
        "            }\n",
        "        })\n",
        "    \n",
        "    return retrieved_chunks\n",
        "\n",
        "# Example: Search within specific document with filters\n",
        "query_vec = np.random.rand(1536).tolist()\n",
        "results = enhanced_rag_retrieval(\n",
        "    client,\n",
        "    query_vec,\n",
        "    filters_dict={'document_id': 'ai_guide'},\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "print(\"ğŸ¯ Enhanced RAG Retrieval:\\n\")\n",
        "for i, chunk in enumerate(results, 1):\n",
        "    print(f\"{i}. {chunk['metadata']['title']}\")\n",
        "    print(f\"   Distance: {chunk['metadata']['distance']:.4f}\")\n",
        "    print(f\"   Tags: {', '.join(chunk['metadata']['tags'])}\")\n",
        "    print(f\"   Words: {chunk['metadata']['word_count']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pattern 3: Complete RAG Pipeline\n",
        "\n",
        "Here's a full end-to-end RAG system implementation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleRAGSystem:\n",
        "    \"\"\"\n",
        "    Complete RAG system using Weaviate.\n",
        "    \n",
        "    This class handles:\n",
        "    - Document ingestion\n",
        "    - Vector search retrieval\n",
        "    - Context formatting\n",
        "    - Complete query pipeline\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, client, collection_name=\"RAGChunk\"):\n",
        "        self.client = client\n",
        "        self.collection_name = collection_name\n",
        "        self.collection = client.collections.get(collection_name)\n",
        "    \n",
        "    def add_documents(self, documents, get_embedding_func):\n",
        "        \"\"\"\n",
        "        Add documents to the vector database.\n",
        "        \n",
        "        Args:\n",
        "            documents: List of dicts with 'content', 'metadata'\n",
        "            get_embedding_func: Function to generate embeddings\n",
        "        \"\"\"\n",
        "        print(f\"ğŸ“¥ Adding {len(documents)} documents...\")\n",
        "        \n",
        "        with self.collection.batch.dynamic() as batch:\n",
        "            for doc in documents:\n",
        "                # Generate embedding\n",
        "                embedding = get_embedding_func(doc['content'])\n",
        "                \n",
        "                # Prepare properties\n",
        "                properties = {\n",
        "                    'content': doc['content'],\n",
        "                    'chunk_id': doc.get('id', str(uuid.uuid4())),\n",
        "                    'document_id': doc['metadata'].get('document_id', ''),\n",
        "                    'title': doc['metadata'].get('title', ''),\n",
        "                    'section_name': doc['metadata'].get('section', ''),\n",
        "                    'parent_chunk_id': '',\n",
        "                    'chunk_level': 'paragraph',\n",
        "                    'chunk_index': 0,\n",
        "                    'word_count': len(doc['content'].split()),\n",
        "                    'has_code': False,\n",
        "                    'has_table': False,\n",
        "                    'tags': doc['metadata'].get('tags', []),\n",
        "                    'created_at': datetime.now().isoformat(),\n",
        "                }\n",
        "                \n",
        "                batch.add_object(properties=properties, vector=embedding)\n",
        "        \n",
        "        print(\"âœ… Documents added successfully\")\n",
        "    \n",
        "    def retrieve(self, query, get_embedding_func, top_k=5, filters=None):\n",
        "        \"\"\"\n",
        "        Retrieve relevant documents.\n",
        "        \n",
        "        Args:\n",
        "            query: User's question\n",
        "            get_embedding_func: Function to generate query embedding\n",
        "            top_k: Number of results\n",
        "            filters: Optional Weaviate filters\n",
        "        \n",
        "        Returns:\n",
        "            List of relevant documents with metadata\n",
        "        \"\"\"\n",
        "        # Generate query embedding\n",
        "        query_embedding = get_embedding_func(query)\n",
        "        \n",
        "        # Search\n",
        "        response = self.collection.query.near_vector(\n",
        "            near_vector=query_embedding,\n",
        "            filters=filters,\n",
        "            limit=top_k,\n",
        "            return_metadata=['distance']\n",
        "        )\n",
        "        \n",
        "        # Format results\n",
        "        results = []\n",
        "        for obj in response.objects:\n",
        "            results.append({\n",
        "                'content': obj.properties['content'],\n",
        "                'title': obj.properties['title'],\n",
        "                'section': obj.properties['section_name'],\n",
        "                'distance': obj.metadata.distance,\n",
        "                'tags': obj.properties.get('tags', []),\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def format_context(self, retrieved_chunks):\n",
        "        \"\"\"Format retrieved chunks into context for LLM.\"\"\"\n",
        "        context_parts = []\n",
        "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
        "            context_parts.append(\n",
        "                f\"[Source {i}: {chunk['title']}]\\\\n{chunk['content']}\"\n",
        "            )\n",
        "        return \"\\\\n\\\\n\".join(context_parts)\n",
        "    \n",
        "    def query(self, question, get_embedding_func, llm_func, top_k=5, filters=None):\n",
        "        \"\"\"\n",
        "        Complete RAG query: retrieve + generate.\n",
        "        \n",
        "        Args:\n",
        "            question: User's question\n",
        "            get_embedding_func: Embedding function\n",
        "            llm_func: LLM generation function\n",
        "            top_k: Number of chunks to retrieve\n",
        "            filters: Optional filters\n",
        "        \n",
        "        Returns:\n",
        "            Dict with answer and sources\n",
        "        \"\"\"\n",
        "        # Step 1: Retrieve relevant context\n",
        "        context_chunks = self.retrieve(question, get_embedding_func, top_k, filters)\n",
        "        \n",
        "        # Step 2: Format context\n",
        "        context = self.format_context(context_chunks)\n",
        "        \n",
        "        # Step 3: Build prompt\n",
        "        prompt = f\"\"\"Answer the question based on the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        \n",
        "        # Step 4: Generate answer\n",
        "        answer = llm_func(prompt)\n",
        "        \n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'sources': context_chunks,\n",
        "            'num_sources': len(context_chunks),\n",
        "        }\n",
        "\n",
        "# Initialize RAG system\n",
        "print(\"\\\\nğŸš€ Complete RAG System initialized!\")\n",
        "print(\"\\\\nUsage example:\")\n",
        "print(\"rag = SimpleRAGSystem(client)\")\n",
        "print(\"result = rag.query('What is machine learning?', embedding_func, llm_func)\")\n",
        "print(\"print(result['answer'])\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Advanced Features\n",
        "\n",
        "### Aggregations & Analytics\n",
        "\n",
        "Monitor your RAG system's health and usage:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get collection statistics\n",
        "def get_collection_stats(client, collection_name):\n",
        "    \"\"\"Get comprehensive statistics about your collection.\"\"\"\n",
        "    collection = client.collections.get(collection_name)\n",
        "    \n",
        "    # Total count\n",
        "    agg = collection.aggregate.over_all(total_count=True)\n",
        "    total = agg.total_count or 0\n",
        "    \n",
        "    # Count by document\n",
        "    doc_filter = Filter.by_property(\"document_id\").equal(\"ai_guide\")\n",
        "    agg_doc = collection.aggregate.over_all(\n",
        "        filters=doc_filter,\n",
        "        total_count=True\n",
        "    )\n",
        "    \n",
        "    stats = {\n",
        "        \"total_chunks\": total,\n",
        "        \"ai_guide_chunks\": agg_doc.total_count or 0,\n",
        "    }\n",
        "    \n",
        "    return stats\n",
        "\n",
        "# Get stats\n",
        "stats = get_collection_stats(client, \"RAGChunk\")\n",
        "print(\"ğŸ“Š Collection Statistics:\\n\")\n",
        "print(f\"   Total Chunks: {stats['total_chunks']}\")\n",
        "print(f\"   AI Guide Chunks: {stats['ai_guide_chunks']}\")\n",
        "\n",
        "# List all collections\n",
        "print(\"\\\\nğŸ“š Available Collections:\")\n",
        "collections = client.collections.list_all()\n",
        "for name in collections.keys():\n",
        "    print(f\"   - {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Best Practices & Optimization\n",
        "\n",
        "### Schema Design Best Practices\n",
        "\n",
        "```python\n",
        "# Essential metadata for RAG systems\n",
        "essential_metadata = {\n",
        "    # Identity\n",
        "    \"chunk_id\": \"unique identifier\",\n",
        "    \"document_id\": \"parent document\",\n",
        "    \n",
        "    # Hierarchy\n",
        "    \"level\": \"document/section/paragraph\",\n",
        "    \"parent_id\": \"parent chunk reference\",\n",
        "    \"chunk_index\": \"position in document\",\n",
        "    \n",
        "    # Content characteristics\n",
        "    \"content_type\": \"text/code/table\",\n",
        "    \"language\": \"en/es/fr\",\n",
        "    \"word_count\": 150,\n",
        "    \n",
        "    # Temporal\n",
        "    \"created_at\": \"timestamp\",\n",
        "    \"updated_at\": \"timestamp\",\n",
        "    \n",
        "    # Quality & relevance\n",
        "    \"quality_score\": 0.95,\n",
        "    \n",
        "    # Semantic tags\n",
        "    \"tags\": [\"topic1\", \"topic2\"],\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunking Strategies for RAG\n",
        "\n",
        "| Strategy | Chunk Size | Best For | Overlap |\n",
        "|----------|-----------|----------|---------|\n",
        "| **Fixed-size** | 256-512 tokens | General purpose | 10-20% |\n",
        "| **Semantic** | Variable | Preserving meaning | Paragraph boundaries |\n",
        "| **Hierarchical** | Multi-level | Complex documents | Natural structure |\n",
        "| **Sentence** | 1-3 sentences | Q&A systems | 1 sentence |\n",
        "\n",
        "### Embedding Model Choices\n",
        "\n",
        "| Model | Dimensions | Speed | Quality | Cost |\n",
        "|-------|-----------|-------|---------|------|\n",
        "| OpenAI text-embedding-3-small | 1536 | Fast | Good | Low |\n",
        "| OpenAI text-embedding-3-large | 3072 | Medium | Excellent | Medium |\n",
        "| Cohere embed-v3 | 1024 | Fast | Good | Low |\n",
        "| Sentence Transformers (local) | 384-768 | Very Fast | Good | Free |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Optimization Tips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tip 1: Don't retrieve unnecessary data\n",
        "response = rag_chunks.query.near_vector(\n",
        "    near_vector=np.random.rand(1536).tolist(),\n",
        "    limit=5,\n",
        "    include_vector=False,  # Don't return embeddings unless needed\n",
        "    return_properties=[\"title\", \"content\"],  # Only specific properties\n",
        "    return_metadata=['distance']  # Only needed metadata\n",
        ")\n",
        "print(\"âœ… Tip 1: Minimize data transfer\\n\")\n",
        "\n",
        "# Tip 2: Use appropriate HNSW settings\n",
        "hnsw_configs = {\n",
        "    \"high_quality\": {\n",
        "        \"ef_construction\": 256,\n",
        "        \"max_connections\": 128,\n",
        "        \"use_case\": \"Maximum recall, slower indexing\"\n",
        "    },\n",
        "    \"balanced\": {\n",
        "        \"ef_construction\": 128,\n",
        "        \"max_connections\": 64,\n",
        "        \"use_case\": \"Good balance (recommended)\"\n",
        "    },\n",
        "    \"fast_indexing\": {\n",
        "        \"ef_construction\": 64,\n",
        "        \"max_connections\": 32,\n",
        "        \"use_case\": \"Fast writes, lower recall\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"âœ… Tip 2: Choose HNSW config based on needs:\")\n",
        "for config, params in hnsw_configs.items():\n",
        "    print(f\"   {config}: {params['use_case']}\")\n",
        "\n",
        "# Tip 3: Batch operations\n",
        "print(\"\\\\nâœ… Tip 3: Always use batching for bulk operations\")\n",
        "print(\"   - Dynamic batching for most cases\")\n",
        "print(\"   - Fixed-size for control over batch size\")\n",
        "\n",
        "# Tip 4: Filter early\n",
        "print(\"\\\\nâœ… Tip 4: Apply filters to reduce search space\")\n",
        "print(\"   - Filter by document_id, date, category\")\n",
        "print(\"   - Combine multiple filters for precision\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Common Pitfalls to Avoid\n",
        "\n",
        "| âŒ Mistake | âœ… Solution |\n",
        "|-----------|-----------|\n",
        "| Not using batching | Use batch operations for >10 objects |\n",
        "| Retrieving too many results | Limit to 5-10 chunks for LLM context |\n",
        "| Ignoring metadata | Rich metadata enables better filtering |\n",
        "| No chunk overlap | Add 10-20% overlap to preserve context |\n",
        "| Wrong distance metric | Use cosine for normalized vectors |\n",
        "| No error handling | Always check batch results |\n",
        "| Forgetting to close client | Always close connections |\n",
        "| Not monitoring performance | Track query latency and recall |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RAG System Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    RAG SYSTEM FLOW                      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "1. INGESTION PHASE\n",
        "   Documents â†’ Chunking â†’ Embeddings â†’ Weaviate\n",
        "   \n",
        "2. RETRIEVAL PHASE\n",
        "   User Query â†’ Embedding â†’ Vector Search â†’ Top K Chunks\n",
        "   \n",
        "3. GENERATION PHASE\n",
        "   Context + Query â†’ LLM â†’ Answer + Sources\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚              RECOMMENDED STACK                          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "Vector DB:     Weaviate\n",
        "Embeddings:    OpenAI text-embedding-3-small\n",
        "LLM:           GPT-4, Claude, or Llama\n",
        "Chunking:      LangChain or custom\n",
        "Framework:     LlamaIndex or LangChain (optional)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Debugging & Monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable logging for debugging\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger('weaviate')\n",
        "\n",
        "# Check client status\n",
        "print(f\"ğŸ” Client Status:\")\n",
        "print(f\"   Connected: {client.is_ready()}\")\n",
        "\n",
        "# List all collections\n",
        "collections = client.collections.list_all()\n",
        "print(f\"\\nğŸ“š Collections in database:\")\n",
        "for name, config in collections.items():\n",
        "    print(f\"   - {name}\")\n",
        "\n",
        "# Get schema details for a collection\n",
        "collection = client.collections.get(\"RAGChunk\")\n",
        "print(f\"\\nâš™ï¸  RAGChunk Collection Config:\")\n",
        "print(f\"   Name: RAGChunk\")\n",
        "print(f\"   Properties: chunk_id, content, title, document_id, etc.\")\n",
        "print(f\"   Vector Index: HNSW with cosine distance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Additional Resources\n",
        "\n",
        "### Official Documentation\n",
        "- **[Weaviate Documentation](https://weaviate.io/developers/weaviate)** - Complete reference\n",
        "- **[Python Client Docs](https://weaviate.io/developers/weaviate/client-libraries/python)** - Python API reference\n",
        "- **[Vector Index Guide](https://weaviate.io/developers/weaviate/concepts/vector-index)** - Deep dive into HNSW\n",
        "\n",
        "### RAG-Specific Resources\n",
        "- **[RAG Patterns & Best Practices](https://weaviate.io/blog/rag-evaluation)** - Evaluation techniques\n",
        "- **[Advanced RAG Techniques](https://weaviate.io/blog/advanced-rag)** - Multi-stage retrieval, reranking\n",
        "- **[Hybrid Search Explained](https://weaviate.io/blog/hybrid-search-explained)** - BM25 + Vector\n",
        "\n",
        "### Community & Support\n",
        "- **[Weaviate Slack Community](https://weaviate.io/slack)** - Active community\n",
        "- **[GitHub Discussions](https://github.com/weaviate/weaviate/discussions)** - Q&A and feature requests\n",
        "- **[Stack Overflow](https://stackoverflow.com/questions/tagged/weaviate)** - Tagged questions\n",
        "\n",
        "### Tutorials & Examples\n",
        "- **[Weaviate Recipes](https://github.com/weaviate/recipes)** - Code examples\n",
        "- **[RAG Notebook Examples](https://github.com/weaviate/weaviate-examples)** - End-to-end examples\n",
        "- **[Vector Database Comparison](https://weaviate.io/blog/vector-database-comparison)** - Why choose Weaviate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Connection Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Always close the client when done\n",
        "# Uncomment the line below when you're finished with the tutorial\n",
        "# client.close()\n",
        "# print(\"âœ… Client connection closed\")\n",
        "\n",
        "print(\"âš ï¸  Remember to close the client when finished: client.close()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary & Next Steps\n",
        "\n",
        "### What You've Learned\n",
        "\n",
        "âœ… **Fundamentals**\n",
        "- What Weaviate is and why it's perfect for RAG\n",
        "- Vector databases and semantic search concepts\n",
        "- Collections, properties, and schemas\n",
        "\n",
        "âœ… **Core Operations**\n",
        "- Connecting to Weaviate (Cloud & Local)\n",
        "- CRUD operations (Create, Read, Update, Delete)\n",
        "- Vector search (near_vector, near_text)\n",
        "- Filtering and hybrid search\n",
        "\n",
        "âœ… **Advanced Features**\n",
        "- Batch operations for efficiency\n",
        "- RAG-specific retrieval patterns\n",
        "- Hierarchical and metadata-enhanced retrieval\n",
        "- Aggregations and analytics\n",
        "\n",
        "âœ… **Best Practices**\n",
        "- Schema design for RAG\n",
        "- Chunking strategies\n",
        "- Performance optimization\n",
        "- Monitoring and debugging\n",
        "\n",
        "### Your Next Steps\n",
        "\n",
        "1. **Integrate Embeddings**\n",
        "   ```python\n",
        "   # Example with OpenAI\n",
        "   import openai\n",
        "   \n",
        "   def get_embedding(text):\n",
        "       response = openai.embeddings.create(\n",
        "           model=\"text-embedding-3-small\",\n",
        "           input=text\n",
        "       )\n",
        "       return response.data[0].embedding\n",
        "   ```\n",
        "\n",
        "2. **Implement Chunking**\n",
        "   - Use LangChain's text splitters\n",
        "   - Or implement custom chunking logic\n",
        "   - Consider overlap and hierarchy\n",
        "\n",
        "3. **Build Your RAG Pipeline**\n",
        "   - Ingest your documents\n",
        "   - Set up retrieval with filters\n",
        "   - Connect to an LLM (GPT-4, Claude, etc.)\n",
        "   - Implement evaluation metrics\n",
        "\n",
        "4. **Optimize & Monitor**\n",
        "   - Track query latency\n",
        "   - Measure retrieval quality (recall, precision)\n",
        "   - A/B test different chunking strategies\n",
        "   - Tune HNSW parameters\n",
        "\n",
        "5. **Production Considerations**\n",
        "   - Set up proper error handling\n",
        "   - Implement caching\n",
        "   - Monitor costs (embeddings, LLM calls)\n",
        "   - Set up logging and alerting\n",
        "\n",
        "### Quick Reference Card\n",
        "\n",
        "```python\n",
        "# Connect\n",
        "client = weaviate.connect_to_weaviate_cloud(...)\n",
        "\n",
        "# Create Collection\n",
        "client.collections.create(name=\"MyCollection\", properties=[...])\n",
        "\n",
        "# Insert Data\n",
        "collection.data.insert(properties={...}, vector=[...])\n",
        "\n",
        "# Batch Insert\n",
        "with collection.batch.dynamic() as batch:\n",
        "    batch.add_object(properties={...}, vector=[...])\n",
        "\n",
        "# Vector Search\n",
        "collection.query.near_vector(near_vector=[...], limit=5)\n",
        "\n",
        "# Filtered Search\n",
        "collection.query.near_vector(\n",
        "    near_vector=[...],\n",
        "    filters=Filter.by_property(\"field\").equal(\"value\"),\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "# Get Stats\n",
        "collection.aggregate.over_all(total_count=True)\n",
        "\n",
        "# Close\n",
        "client.close()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ‰ Congratulations!\n",
        "\n",
        "You now have a comprehensive understanding of Weaviate for RAG systems. You're ready to build production-quality retrieval systems that can:\n",
        "\n",
        "- Handle millions of documents\n",
        "- Perform fast semantic search\n",
        "- Combine vector search with metadata filtering\n",
        "- Scale efficiently with your application\n",
        "\n",
        "**Happy building!** ğŸš€\n",
        "\n",
        "---\n",
        "\n",
        "*This tutorial was created for learning Weaviate in the context of building RAG systems. For the latest updates, always refer to the [official Weaviate documentation](https://weaviate.io/developers/weaviate).*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
